{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"vQTL Analysis Better tools to analyze the varying genes interacting with particular phenotypes. Core Contributers Ann E. Stapleton : University of North Carolina Wilmington Department of Biology and Marine Biology Robert Corty : Yishi Wang : University of North Carolina Wilmington Department of Mathematics and Statistics Student Contributers Austin Gratton : University of North Carolina Wilmington azg5169@uncw.edu(opens in new tab) grattonauz@gmail.com(opens in new tab) Michael Copeland : University of North Carolina Wilmington mrc5353@uncw.edu(opens in new tab) michaelrcopeland74@gmail.com(opens in new tab) Purpose Better combinations of genes within corn crops due to genetic modification can result in a better harvest. These combinations of genes can further be altered to maximize the harvest in various environment conditions such as low rainfall/poor irrigation, low nitrogen levels, or with the presence of a pathogen or in almagamation of these factors. Farmers can assess the environmental conditions and thus, determine the most favorable genotype to grow in their landscape for the best crop yield. Reference Need references","title":"Home"},{"location":"#vqtl-analysis","text":"Better tools to analyze the varying genes interacting with particular phenotypes.","title":"vQTL Analysis"},{"location":"#core-contributers","text":"Ann E. Stapleton : University of North Carolina Wilmington Department of Biology and Marine Biology Robert Corty : Yishi Wang : University of North Carolina Wilmington Department of Mathematics and Statistics","title":"Core Contributers"},{"location":"#student-contributers","text":"Austin Gratton : University of North Carolina Wilmington azg5169@uncw.edu(opens in new tab) grattonauz@gmail.com(opens in new tab) Michael Copeland : University of North Carolina Wilmington mrc5353@uncw.edu(opens in new tab) michaelrcopeland74@gmail.com(opens in new tab)","title":"Student Contributers"},{"location":"#purpose","text":"Better combinations of genes within corn crops due to genetic modification can result in a better harvest. These combinations of genes can further be altered to maximize the harvest in various environment conditions such as low rainfall/poor irrigation, low nitrogen levels, or with the presence of a pathogen or in almagamation of these factors. Farmers can assess the environmental conditions and thus, determine the most favorable genotype to grow in their landscape for the best crop yield.","title":"Purpose"},{"location":"#reference","text":"Need references","title":"Reference"},{"location":"Data/","text":"th { font-size: 8px } td{ font-size: 10px } Data Collected qPCR Data Not sure what this looks like vQTL Data The data we used for the vQTL data was all in one csv file known as the Manching Stress Product Data. This file conatained a phenotype which is the height of the corn crop. There are 8 columns of different environment combinations of low water, low nitrogen, or presence of a pathogen. There is an environment column numbering the different combinations from 1-8. These combinations are either 1 or 0. Then there is 3235 columns of different gene names. These are either A, B or NA. The for the rows we have a row indicating the chromosones that the genes are on. There are 10 different chromosones. There is another row that indicates the distance the gene is on the chromosone. The next 6672 rows are different tests with varying gene combinations and varying envronmental combinations. The data is very similar however not the same for each environmental combination, and for some environmental combinations more tests have been done. Height Low.Water Low.Nitrogen Pathogen Low_W_N Low_W_P Low_N_P All None Env gpm27 tub1 gpm113b gpm705a gpm325a dmt103b gpm699d gpm319 IDP1447 bnl5.62a 1 1 1 1 1 1 1 1 1 1 0.01 0.81 0.82 0.84 4.97 6.89 8.67 14.82 19.92 21.12 71 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 79 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 48 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 85 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 36 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 72 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 58 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 76 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 68 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 76 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 81 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 41 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 90 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 75 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 82 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 79 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 80 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 83 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 85 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 65 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 83 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 61 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 86 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 53 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 40 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 52 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 84 0 0 0 0 0 0 0 1 1 B B B B B B B B B -","title":"Data"},{"location":"Data/#data-collected","text":"","title":"Data Collected"},{"location":"Data/#qpcr-data","text":"Not sure what this looks like","title":"qPCR Data"},{"location":"Data/#vqtl-data","text":"The data we used for the vQTL data was all in one csv file known as the Manching Stress Product Data. This file conatained a phenotype which is the height of the corn crop. There are 8 columns of different environment combinations of low water, low nitrogen, or presence of a pathogen. There is an environment column numbering the different combinations from 1-8. These combinations are either 1 or 0. Then there is 3235 columns of different gene names. These are either A, B or NA. The for the rows we have a row indicating the chromosones that the genes are on. There are 10 different chromosones. There is another row that indicates the distance the gene is on the chromosone. The next 6672 rows are different tests with varying gene combinations and varying envronmental combinations. The data is very similar however not the same for each environmental combination, and for some environmental combinations more tests have been done. Height Low.Water Low.Nitrogen Pathogen Low_W_N Low_W_P Low_N_P All None Env gpm27 tub1 gpm113b gpm705a gpm325a dmt103b gpm699d gpm319 IDP1447 bnl5.62a 1 1 1 1 1 1 1 1 1 1 0.01 0.81 0.82 0.84 4.97 6.89 8.67 14.82 19.92 21.12 71 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 79 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 48 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 85 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 36 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 72 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 58 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 76 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 68 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 76 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 81 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 41 0 0 0 0 0 0 0 1 1 B B A - B B B - A A 90 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 75 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 82 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 79 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 80 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 83 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 85 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 65 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 83 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 61 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 86 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 53 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 40 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 52 0 0 0 0 0 0 0 1 1 B B B B B B B B B - 84 0 0 0 0 0 0 0 1 1 B B B B B B B B B -","title":"vQTL Data"},{"location":"Jobs%20on%20HPC/","text":"The Batch Script and Jobs This section of the tutorial will assume some basic knowledge of how to log into the HPC via ssh and to navigate directories via Command Line or Terminal. Feel free to go to the last section of the GitHub tutorial for a brief refresher of the ssh log-in process. Likewise, we will briefly discuss how to navigate directories in this tutorial, but more information is given in the \"Reminders and Useful Commands\" tutorial. Batch Script Unlike a personal computer the HPC will not simply run an R or Python script. The high performance computer must be \"told\" what to do. This is because endusers have the ability to request how much and what type of resources are utilized when a job is submitted through the system. For example, you will have one option to choose how many cores the HPC uses to process your request and yet another option is to have the system email you when a job is submitted or completed. The batch script is what communicates your requests to the HPC. You may find an example batch script below: #!/bin/bash (Never change this line) # # #SBATCH -J job_name #SBATCH -o job_name.%j.out #SBATCH -N number_of_nodes #SBATCH -n total_number_of_MPI_tasks #SBATCH -p process_queues #SBATCH -t max_time #SBATCH -A Envriotype #SBATCH --mail-user=<email@host.com> #SBATCH --mail-type=all #------------------------------------------------------ mkdir -p output Rscript --verbose ./script_to_be_run.R > ./output.Rout Notice, however, you must change a few generic identifiers with your own specifics. For example, \"job_name\" should be replaced with a name which is less than 8 characters long that will help you identify it in the queue. The expressions \"number_of_nodes\" (N) and \"total_number_of_MPI_tasks\" (n) must both be replaced with at least 1. The value \"max_time\" must be in the format hh:mm:ss, where hh is the number of hours, mm is the number of minutes, and ss is the number of seconds you think are needed for your job to complete. The expression \" email@host.com \" should be replaced with your preferred email address (without the arrows on the ends), and \"script_to_be_run.R\" should be replaced with the name of a file in the same folder as the batch script you wish the HPC to run. The only line which requires much thought is the one where you choose how to replace \"process_queues\". The typical selection is to replace it with \"normal\"; but you may also choose to select queues like \"large\", \"long\", or \"development\". There are several other options in addition to these, but know that the process queue you choose will determine when your job begins, how long it may run before being automatically terminated, and the number of compute resources available to you. If you wish to know more about batch scripts, there are plenty of other tutorials online. One great tutorial is provided by Cornell University's Center for Advanced Computing. This tutorial provides a good introduction to how Stampede2 operates and gives several great examples of batch scripts and other useful SLURM codes. Push and Pull Requests When you wish to edit a batch script, open RStudio and go to the EnviroTyping Project created earlier. You can edit batch scripts in RStudio just like any other file, so it will be assumed you are working in this IDE. You may choose to edit an existing batch script or create a new one; ensure the file name is in the form of file_name.sh because the .sh extension is one way the HPC knows how to read the script. Once you finish editing the batch script (or any other file), you must commit the change to your GitHub repository and submit a push request. First, open RStudio and look around the top-middle of the RStudio window for a smaller icon with a green \"+\", red \"-\", and grey \"G\" symbol vertically stacked. Click on the symbol, then select \"Commit\". This starts the staging process. A new window should open where you select the files you want to stage, select all the files you want to merge into your repository and write a short message that describes the changes made in each file. Once all changes have been staged, click the green arrow that says \"Push\" to add the new changes to your GitHub repository. If there are no complications, your repository should be updated in less than a minute. Next, you must make sure the repository cloned to your Stampede2 drive is updated as well. Log-in to Stampede2 as before and enter the command git pull . If there are no complications and few files to update, your command line editor will give descriptions of the files added, changed, or deleted in a matter of a few seconds. Make sure to update each drive ($home, $work, etc.) in Stampede2 as necessary. This process is the same for any file -- not just batch scripts. Once your changes have been pushed and pulled without issue, you may continue to submit your batch script to the HPC for processing. Job Submittal Process Assuming you are already logged into the system, there are three steps to submitting a job request to Stampede2. First, load the modules needed to run the job. Second, navigate to the directory of the batch script you wish to submit. Third, submit the batch script. Loading modules requires the use of a single command: module load <module> or ml <module> . EnviroTyping requires the extensive use of R, so you will more than likely need to use ml Rstats . If need be, you can also load Python or other modules in similar manner. Navigating directories via command line editors is simple. If you're unsure of the layout of the folders you can see all the folders and files on the current level of the directory by entering the command ls . You can then go down a level by typing the command cd <folder_name> . If you accidentally go into the wrong folder, the command cd .. will take you back up one level of your directory. You may repeat this process until you find the location of the batch script. Finally, you are ready to submit the job request! Type sbatch <file_name.sh> . Stampede should output information about the job request, such as the request number and some other identifying information. From here you may do a number of things, such as check on the status of the request or even cancel the job. More information about your options is found in the \"Reminders and Useful Commands\" tutorial. Stampede Rules and Practices One of the most important rules for using Stampede2 resources is to practice good citizenship. Once you create your TACC account, you become one of the thousands of users sharing the same compute power so it's best to start learning to \"do unto others as you'd have them do unto you.\" The following are some basic rules to follow. First, TACC recommends submitting job requests in $WORK. This is accomplished by entering the command cdw at the beginning of your session before you follow the steps to submit a job. Use $HOME (or cdh ) to manage files. Don't submit an R (or other program) script without first submitting a batch script because you will inadvertently drain compute resources from the log-in nodes. Doing so will result in a nicely worded email from TACC. Second, when requesting time in your batch script, make sure you request an appropriate amount of time for your job and the appropriate process queue. For example, if a job should complete in 2 hours, don't request 10 or ask for the \"long\" process queue. It would be best to ask for, maybe, 5 hours and the \"normal\" queue. This increases the chances your job will be accepted sooner and allows other resources to be properly allocated. Stampede2 uses a formula that \"bills\" users for the amount of resources they need for a job; however, for most users, the workings of the computation is irrelevant, so long as they know it exists. In other words, the more time and nodes you request the longer you will wait for the job to start processing. You can read more about the various queues and their properties in the User Guide . There are many aspects to utilizing the HPC, so it never hurts to ask if you have any questions about the process or how to be a good citizen. Never hesitate to ask for help if you can't find what you need in the User Guide or another resource. RStudio Interface on HPC If there comes a time when you'd like to work in the RStudio IDE while staying connected to Stampede and its compute resources, you can do that. Enter the command sbatch /share/doc/slurm/job.Rstudio after you are logged into Stampede, and give the HPC a moment to process the input. You will be given a URL to connect to the web interface of RStudio where you will be required to submit your TACC log-in credentials. After submitting them correctly you will then have access to the familiar IDE with a cleaner visualization of your directories and any output which may be produced. This is a helpful tool if you would like to keep an eye on any work as it is processed.","title":"Jobs on HPC"},{"location":"Jobs%20on%20HPC/#the-batch-script-and-jobs","text":"This section of the tutorial will assume some basic knowledge of how to log into the HPC via ssh and to navigate directories via Command Line or Terminal. Feel free to go to the last section of the GitHub tutorial for a brief refresher of the ssh log-in process. Likewise, we will briefly discuss how to navigate directories in this tutorial, but more information is given in the \"Reminders and Useful Commands\" tutorial.","title":"The Batch Script and Jobs"},{"location":"Jobs%20on%20HPC/#batch-script","text":"Unlike a personal computer the HPC will not simply run an R or Python script. The high performance computer must be \"told\" what to do. This is because endusers have the ability to request how much and what type of resources are utilized when a job is submitted through the system. For example, you will have one option to choose how many cores the HPC uses to process your request and yet another option is to have the system email you when a job is submitted or completed. The batch script is what communicates your requests to the HPC. You may find an example batch script below: #!/bin/bash (Never change this line) # # #SBATCH -J job_name #SBATCH -o job_name.%j.out #SBATCH -N number_of_nodes #SBATCH -n total_number_of_MPI_tasks #SBATCH -p process_queues #SBATCH -t max_time #SBATCH -A Envriotype #SBATCH --mail-user=<email@host.com> #SBATCH --mail-type=all #------------------------------------------------------ mkdir -p output Rscript --verbose ./script_to_be_run.R > ./output.Rout Notice, however, you must change a few generic identifiers with your own specifics. For example, \"job_name\" should be replaced with a name which is less than 8 characters long that will help you identify it in the queue. The expressions \"number_of_nodes\" (N) and \"total_number_of_MPI_tasks\" (n) must both be replaced with at least 1. The value \"max_time\" must be in the format hh:mm:ss, where hh is the number of hours, mm is the number of minutes, and ss is the number of seconds you think are needed for your job to complete. The expression \" email@host.com \" should be replaced with your preferred email address (without the arrows on the ends), and \"script_to_be_run.R\" should be replaced with the name of a file in the same folder as the batch script you wish the HPC to run. The only line which requires much thought is the one where you choose how to replace \"process_queues\". The typical selection is to replace it with \"normal\"; but you may also choose to select queues like \"large\", \"long\", or \"development\". There are several other options in addition to these, but know that the process queue you choose will determine when your job begins, how long it may run before being automatically terminated, and the number of compute resources available to you. If you wish to know more about batch scripts, there are plenty of other tutorials online. One great tutorial is provided by Cornell University's Center for Advanced Computing. This tutorial provides a good introduction to how Stampede2 operates and gives several great examples of batch scripts and other useful SLURM codes.","title":"Batch Script"},{"location":"Jobs%20on%20HPC/#push-and-pull-requests","text":"When you wish to edit a batch script, open RStudio and go to the EnviroTyping Project created earlier. You can edit batch scripts in RStudio just like any other file, so it will be assumed you are working in this IDE. You may choose to edit an existing batch script or create a new one; ensure the file name is in the form of file_name.sh because the .sh extension is one way the HPC knows how to read the script. Once you finish editing the batch script (or any other file), you must commit the change to your GitHub repository and submit a push request. First, open RStudio and look around the top-middle of the RStudio window for a smaller icon with a green \"+\", red \"-\", and grey \"G\" symbol vertically stacked. Click on the symbol, then select \"Commit\". This starts the staging process. A new window should open where you select the files you want to stage, select all the files you want to merge into your repository and write a short message that describes the changes made in each file. Once all changes have been staged, click the green arrow that says \"Push\" to add the new changes to your GitHub repository. If there are no complications, your repository should be updated in less than a minute. Next, you must make sure the repository cloned to your Stampede2 drive is updated as well. Log-in to Stampede2 as before and enter the command git pull . If there are no complications and few files to update, your command line editor will give descriptions of the files added, changed, or deleted in a matter of a few seconds. Make sure to update each drive ($home, $work, etc.) in Stampede2 as necessary. This process is the same for any file -- not just batch scripts. Once your changes have been pushed and pulled without issue, you may continue to submit your batch script to the HPC for processing.","title":"Push and Pull Requests"},{"location":"Jobs%20on%20HPC/#job-submittal-process","text":"Assuming you are already logged into the system, there are three steps to submitting a job request to Stampede2. First, load the modules needed to run the job. Second, navigate to the directory of the batch script you wish to submit. Third, submit the batch script. Loading modules requires the use of a single command: module load <module> or ml <module> . EnviroTyping requires the extensive use of R, so you will more than likely need to use ml Rstats . If need be, you can also load Python or other modules in similar manner. Navigating directories via command line editors is simple. If you're unsure of the layout of the folders you can see all the folders and files on the current level of the directory by entering the command ls . You can then go down a level by typing the command cd <folder_name> . If you accidentally go into the wrong folder, the command cd .. will take you back up one level of your directory. You may repeat this process until you find the location of the batch script. Finally, you are ready to submit the job request! Type sbatch <file_name.sh> . Stampede should output information about the job request, such as the request number and some other identifying information. From here you may do a number of things, such as check on the status of the request or even cancel the job. More information about your options is found in the \"Reminders and Useful Commands\" tutorial.","title":"Job Submittal Process"},{"location":"Jobs%20on%20HPC/#stampede-rules-and-practices","text":"One of the most important rules for using Stampede2 resources is to practice good citizenship. Once you create your TACC account, you become one of the thousands of users sharing the same compute power so it's best to start learning to \"do unto others as you'd have them do unto you.\" The following are some basic rules to follow. First, TACC recommends submitting job requests in $WORK. This is accomplished by entering the command cdw at the beginning of your session before you follow the steps to submit a job. Use $HOME (or cdh ) to manage files. Don't submit an R (or other program) script without first submitting a batch script because you will inadvertently drain compute resources from the log-in nodes. Doing so will result in a nicely worded email from TACC. Second, when requesting time in your batch script, make sure you request an appropriate amount of time for your job and the appropriate process queue. For example, if a job should complete in 2 hours, don't request 10 or ask for the \"long\" process queue. It would be best to ask for, maybe, 5 hours and the \"normal\" queue. This increases the chances your job will be accepted sooner and allows other resources to be properly allocated. Stampede2 uses a formula that \"bills\" users for the amount of resources they need for a job; however, for most users, the workings of the computation is irrelevant, so long as they know it exists. In other words, the more time and nodes you request the longer you will wait for the job to start processing. You can read more about the various queues and their properties in the User Guide . There are many aspects to utilizing the HPC, so it never hurts to ask if you have any questions about the process or how to be a good citizen. Never hesitate to ask for help if you can't find what you need in the User Guide or another resource.","title":"Stampede Rules and Practices"},{"location":"Jobs%20on%20HPC/#rstudio-interface-on-hpc","text":"If there comes a time when you'd like to work in the RStudio IDE while staying connected to Stampede and its compute resources, you can do that. Enter the command sbatch /share/doc/slurm/job.Rstudio after you are logged into Stampede, and give the HPC a moment to process the input. You will be given a URL to connect to the web interface of RStudio where you will be required to submit your TACC log-in credentials. After submitting them correctly you will then have access to the familiar IDE with a cleaner visualization of your directories and any output which may be produced. This is a helpful tool if you would like to keep an eye on any work as it is processed.","title":"RStudio Interface on HPC"},{"location":"Launcher%20Module/","text":"Launcher Tutorial Launcher is a utility for performing simple, data parallel, high throughput computing (HTC) workflows on clusters, massively parallel processor (MPP) systems, workgroups of computers, and personal machines. Wilson:2014:LSF:2616498.2616533 This tutorial is assuming the use of the launcher module found in the TACC Lmod's and some familiarity with TACC HPC systems. Purpose There are many types of jobs for running with the launcher module but for the scope of this tutorial we are going to practice with R scripts. You are going to generate some random datasets with R and print them to csv files. Then you will create a launcher file to run 100 jobs on 10 nodes. Finally you will use the launcher module to run all 100 jobs with very little typing involved. Loading Launcher Login to one of the TACC systems such as Stampede2. ssh username@stampede2.tacc.utexas.edu This logs you into your $HOME directory. We typically run analysis in the $WORK directory. Just type following to move to your $WORK directory cdw . To see the modules you currently have loaded: module list To load the launcher module if not listed in the output: module load launcher You can optionally save the launcher module as a default every time you login to the system: module list confirm launcher is loaded then: module save We also need to use the Rstats module. module load Rstats Neccessary Files You need to copy 4 files to use this tutorial: * randcsvGen.R - This is an Rscript that randomly generates 100 csv files using the mtcars dataset. * launcherFile.sh - This bash script will parse all the csv filenames into an array and return them to launcherFile . launcherFile is a list of Rscript command line arguments to run each job. * launcher.slurm - This is the file which actually batches the jobs to different HPC nodes. * headDF.R - A simple R script which prints the top of the dataset read in from each csv file. All these files should be in the same directory in your $WORK directory. Modify launcher.slurm Set LAUNCHER_JOB_FILE to point to your job file. We are going to use Vim, a built-in editor. Type vim launcher.slurm Type i and the prompt at the bottom will say Insert . Use your arrow keys and change <path_to_directory> to the directory you are currently in, which is the $WORK directory if you did not place the files somewhere else. Change <allocation> to your group's allocation for the system. m Then hit Esc and type :wq which tells Vim to save and quit. Generate Some Data The first file you will run is the launcherFile.sh . This will run the R script to generate the csv files and created your launcherFile . Type chmod +x launcherFile.sh to give your user permission to execute. Then type ./launcherFile.sh This might take a few minutes to install the required R packages generating a lot of output in the terminal. You will have 3 new directorys created: * randcsv/ - This is where all the random csv files are stored. * output/ - For the results from the analysis. * error/ - There will be some general warnings that are read as error by R. The launcherFile should also be in your current directory now as well. Submit the Jobs Your ready to submit your batch of 100 jobs. Type sbatch launcher.slurm A new file will be created for the job: slurm-job##.out You can check the status of your jobs. squeue -u <username> When the jobs are done you should have 100 files in the output/ and error/ directories. Remember you can use Vim if you want to open the file or cat <filename> to peak inside. Wrap-up Again you can you launcher to run jobs with python or Agave apps among many other uses. Hopefully you now have some understanding how to use the launcher module to batch many jobs at once. References https://github.com/TACC/launcher#referencing-launcher https://portal.tacc.utexas.edu/user-guides/stampede2#using-modules-to-manage-your-environment","title":"Launcher Module"},{"location":"Launcher%20Module/#launcher-tutorial","text":"Launcher is a utility for performing simple, data parallel, high throughput computing (HTC) workflows on clusters, massively parallel processor (MPP) systems, workgroups of computers, and personal machines. Wilson:2014:LSF:2616498.2616533 This tutorial is assuming the use of the launcher module found in the TACC Lmod's and some familiarity with TACC HPC systems.","title":"Launcher Tutorial"},{"location":"Launcher%20Module/#purpose","text":"There are many types of jobs for running with the launcher module but for the scope of this tutorial we are going to practice with R scripts. You are going to generate some random datasets with R and print them to csv files. Then you will create a launcher file to run 100 jobs on 10 nodes. Finally you will use the launcher module to run all 100 jobs with very little typing involved.","title":"Purpose"},{"location":"Launcher%20Module/#loading-launcher","text":"Login to one of the TACC systems such as Stampede2. ssh username@stampede2.tacc.utexas.edu This logs you into your $HOME directory. We typically run analysis in the $WORK directory. Just type following to move to your $WORK directory cdw . To see the modules you currently have loaded: module list To load the launcher module if not listed in the output: module load launcher You can optionally save the launcher module as a default every time you login to the system: module list confirm launcher is loaded then: module save We also need to use the Rstats module. module load Rstats","title":"Loading Launcher"},{"location":"Launcher%20Module/#neccessary-files","text":"You need to copy 4 files to use this tutorial: * randcsvGen.R - This is an Rscript that randomly generates 100 csv files using the mtcars dataset. * launcherFile.sh - This bash script will parse all the csv filenames into an array and return them to launcherFile . launcherFile is a list of Rscript command line arguments to run each job. * launcher.slurm - This is the file which actually batches the jobs to different HPC nodes. * headDF.R - A simple R script which prints the top of the dataset read in from each csv file. All these files should be in the same directory in your $WORK directory.","title":"Neccessary Files"},{"location":"Launcher%20Module/#modify-launcherslurm","text":"Set LAUNCHER_JOB_FILE to point to your job file. We are going to use Vim, a built-in editor. Type vim launcher.slurm Type i and the prompt at the bottom will say Insert . Use your arrow keys and change <path_to_directory> to the directory you are currently in, which is the $WORK directory if you did not place the files somewhere else. Change <allocation> to your group's allocation for the system. m Then hit Esc and type :wq which tells Vim to save and quit.","title":"Modify launcher.slurm"},{"location":"Launcher%20Module/#generate-some-data","text":"The first file you will run is the launcherFile.sh . This will run the R script to generate the csv files and created your launcherFile . Type chmod +x launcherFile.sh to give your user permission to execute. Then type ./launcherFile.sh This might take a few minutes to install the required R packages generating a lot of output in the terminal. You will have 3 new directorys created: * randcsv/ - This is where all the random csv files are stored. * output/ - For the results from the analysis. * error/ - There will be some general warnings that are read as error by R. The launcherFile should also be in your current directory now as well.","title":"Generate Some Data"},{"location":"Launcher%20Module/#submit-the-jobs","text":"Your ready to submit your batch of 100 jobs. Type sbatch launcher.slurm A new file will be created for the job: slurm-job##.out You can check the status of your jobs. squeue -u <username> When the jobs are done you should have 100 files in the output/ and error/ directories. Remember you can use Vim if you want to open the file or cat <filename> to peak inside.","title":"Submit the Jobs"},{"location":"Launcher%20Module/#wrap-up","text":"Again you can you launcher to run jobs with python or Agave apps among many other uses. Hopefully you now have some understanding how to use the launcher module to batch many jobs at once.","title":"Wrap-up"},{"location":"Launcher%20Module/#references","text":"https://github.com/TACC/launcher#referencing-launcher https://portal.tacc.utexas.edu/user-guides/stampede2#using-modules-to-manage-your-environment","title":"References"},{"location":"Project%20Overview/","text":"Project Overview & Scope Standards and Methods Click here for Genomes to Fields phenotyping handbook guidelines Number of Tests Conducted Year 2014 2015 2016 2017 No. of experiments 1 1 1 1 2014 Experiment put text here 2015 Experiment put text here 2016 Experiment put text here 2017 Experiment put text here 2018 Experiment put text here 2019 Experiment put text here","title":"Project Overview"},{"location":"Project%20Overview/#project-overview-scope","text":"","title":"Project Overview &amp; Scope"},{"location":"Project%20Overview/#standards-and-methods","text":"Click here for Genomes to Fields phenotyping handbook guidelines","title":"Standards and Methods"},{"location":"Project%20Overview/#number-of-tests-conducted","text":"Year 2014 2015 2016 2017 No. of experiments 1 1 1 1","title":"Number of Tests Conducted"},{"location":"Project%20Overview/#2014-experiment","text":"put text here","title":"2014 Experiment"},{"location":"Project%20Overview/#2015-experiment","text":"put text here","title":"2015 Experiment"},{"location":"Project%20Overview/#2016-experiment","text":"put text here","title":"2016 Experiment"},{"location":"Project%20Overview/#2017-experiment","text":"put text here","title":"2017 Experiment"},{"location":"Project%20Overview/#2018-experiment","text":"put text here","title":"2018 Experiment"},{"location":"Project%20Overview/#2019-experiment","text":"put text here","title":"2019 Experiment"},{"location":"Reminders%20and%20Useful%20Commands/","text":"Reminders First, recall that when utilizing the HPC, you are not actually processing information in your local machine. As such, be sure to not type commands only Command Line (on Windows) or Terminal (on Mac OS) recognizes. Stampede2 is a collection of Lustre servers that runs on Linux. The differences between languages may be small, but recognizing there could be a distinction will assist you should you ever need to look for extra help online. Second, it cannot be stressed enough that you must load modules anytime you want to run an analysis through Stampede2 before the batch script is submitted. As mentioned previously, you may do so by typing module load <module> or ml <module> . An example of this statement is ml Rstats . If you don't load the module, you may exit the HPC thinking your jobs are running, only to return at a later time to find the HPC did not know what to do with your batch script. Third, if your job requires the use of packages not already included in your base module, you must install the packages in the HPC first. It is easiest to do this by loading your preferred module, calling it, then installing the necessary packages. For example, if you need to install tidyvese , you would employ the following steps: ml Rstats R # to open R install.packages(\"tidyverse\") q() # to quit R Once you finish installing the packages, you may continue to submit jobs as needed. Fourth, don't forget to push or pull updates from your Git respository. Many headaches are avoided if you get into the habit of typing git pull in the command line as soon as you push an updated file from your local machine and are logged into Stampede2. ## Other Useful Commands Navigating directories through a command line can be daunting the first couple of times you do it, but it becomes easier with time and experience. Here are some basic commands and their effects: ls returns the collection of folders and files in the current level of your directory, cd <directory> will take you to a lower level (subfolder) of the directory, cd .. will take you one level higher, sbatch <batch script filename> submits your job, squeue -u <Stampede2 username> tells you the details of all your current jobs, scancel <job number> cancels the requested job, cat <job output filename> reads the output file of a module (e.g., a .out or .Rout file), and exit disconnects your SSH session to Stampede2. There may be a time when you need to remove existing files or whole folders to free up space in your $HOME or $WORK directories. Doing so is straight-forward, but extreme caution should be practiced because data is not recoverable once removed using these methods. To remove a file: rm <filename> . To remove an empty folder: rmdir <foldername> . To remove a non-empty folder: rmdir -R <foldername> . To remove a non-empty folder but receive a prompt before removal: rmdir -iR <foldername> . It is easy to remove a file when you are in the proper directory already; but if you are not, simply specify the file's full address when using the above commands. Likewise, you may delete multiple files at once by listing the unwanted file names beside each other: rm <filename1> <filename2> . Command line programming is powerful. However, such power is hard to tame without the proper knowledge. If you ever need extra information for working with Stampede2, the Internet is a great resource; but the best help will be given by XSEDE's Help Desk .","title":"Reminders and Useful Commands"},{"location":"Reminders%20and%20Useful%20Commands/#reminders","text":"First, recall that when utilizing the HPC, you are not actually processing information in your local machine. As such, be sure to not type commands only Command Line (on Windows) or Terminal (on Mac OS) recognizes. Stampede2 is a collection of Lustre servers that runs on Linux. The differences between languages may be small, but recognizing there could be a distinction will assist you should you ever need to look for extra help online. Second, it cannot be stressed enough that you must load modules anytime you want to run an analysis through Stampede2 before the batch script is submitted. As mentioned previously, you may do so by typing module load <module> or ml <module> . An example of this statement is ml Rstats . If you don't load the module, you may exit the HPC thinking your jobs are running, only to return at a later time to find the HPC did not know what to do with your batch script. Third, if your job requires the use of packages not already included in your base module, you must install the packages in the HPC first. It is easiest to do this by loading your preferred module, calling it, then installing the necessary packages. For example, if you need to install tidyvese , you would employ the following steps: ml Rstats R # to open R install.packages(\"tidyverse\") q() # to quit R Once you finish installing the packages, you may continue to submit jobs as needed. Fourth, don't forget to push or pull updates from your Git respository. Many headaches are avoided if you get into the habit of typing git pull in the command line as soon as you push an updated file from your local machine and are logged into Stampede2. ## Other Useful Commands Navigating directories through a command line can be daunting the first couple of times you do it, but it becomes easier with time and experience. Here are some basic commands and their effects: ls returns the collection of folders and files in the current level of your directory, cd <directory> will take you to a lower level (subfolder) of the directory, cd .. will take you one level higher, sbatch <batch script filename> submits your job, squeue -u <Stampede2 username> tells you the details of all your current jobs, scancel <job number> cancels the requested job, cat <job output filename> reads the output file of a module (e.g., a .out or .Rout file), and exit disconnects your SSH session to Stampede2. There may be a time when you need to remove existing files or whole folders to free up space in your $HOME or $WORK directories. Doing so is straight-forward, but extreme caution should be practiced because data is not recoverable once removed using these methods. To remove a file: rm <filename> . To remove an empty folder: rmdir <foldername> . To remove a non-empty folder: rmdir -R <foldername> . To remove a non-empty folder but receive a prompt before removal: rmdir -iR <foldername> . It is easy to remove a file when you are in the proper directory already; but if you are not, simply specify the file's full address when using the above commands. Likewise, you may delete multiple files at once by listing the unwanted file names beside each other: rm <filename1> <filename2> . Command line programming is powerful. However, such power is hard to tame without the proper knowledge. If you ever need extra information for working with Stampede2, the Internet is a great resource; but the best help will be given by XSEDE's Help Desk .","title":"Reminders"},{"location":"Updating%20the%20Docs/","text":"Updating the Docs Updating the documentation is easy and should be done as users discover useful tips and tricks along their own workflows. All documentation is stored on GitHub in plain-text at EnviroTyping Docs . Accessing the source Make a working copy of the documentation. Using command line via terminal From your working directory, download the project from GitHub:: git clone https://github.com/TACC/EnviroTyping.git After a change has been made to the master repository, ReadtheDocs automatically builds fresh html documentation hosted on their servers. Using a desktop web browser Browse to EnviroTyping Project on GitHub and click \"Clone or Download\" at the right. You can use the GitHub desktop app or work with the compressed folder using the text editor of your choice. For comprehensive edits you may wish to use the Atom Editor with the markdown preview package enabled with the documentation directory selected as your project folder. For more on MkDocs / Read the Docs, see: MkDocs - Getting Started ReadtheDocs Instructions Forking & Committing Changes Follow the standard git commit process to request changes. For a full introduction see: Fork a Repo Pull request tutorial Git Novie In short, you'll want to create a fork of the repository from the terminal or the GitHub project's website. The repository includes markdown source files which previewing in html on your own machine. Once you've finished with your proposed changes, add & commit the changes to your fork & open a pull request to the master branch at TACC/EnviroTyping/docs. How it Works MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Contact azg5169@uncw.edu","title":"Updating the Docs"},{"location":"Updating%20the%20Docs/#updating-the-docs","text":"Updating the documentation is easy and should be done as users discover useful tips and tricks along their own workflows. All documentation is stored on GitHub in plain-text at EnviroTyping Docs .","title":"Updating the Docs"},{"location":"Updating%20the%20Docs/#accessing-the-source","text":"Make a working copy of the documentation. Using command line via terminal From your working directory, download the project from GitHub:: git clone https://github.com/TACC/EnviroTyping.git After a change has been made to the master repository, ReadtheDocs automatically builds fresh html documentation hosted on their servers. Using a desktop web browser Browse to EnviroTyping Project on GitHub and click \"Clone or Download\" at the right. You can use the GitHub desktop app or work with the compressed folder using the text editor of your choice. For comprehensive edits you may wish to use the Atom Editor with the markdown preview package enabled with the documentation directory selected as your project folder. For more on MkDocs / Read the Docs, see: MkDocs - Getting Started ReadtheDocs Instructions","title":"Accessing the source"},{"location":"Updating%20the%20Docs/#forking-committing-changes","text":"Follow the standard git commit process to request changes. For a full introduction see: Fork a Repo Pull request tutorial Git Novie In short, you'll want to create a fork of the repository from the terminal or the GitHub project's website. The repository includes markdown source files which previewing in html on your own machine. Once you've finished with your proposed changes, add & commit the changes to your fork & open a pull request to the master branch at TACC/EnviroTyping/docs.","title":"Forking &amp; Committing Changes"},{"location":"Updating%20the%20Docs/#how-it-works","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Contact azg5169@uncw.edu","title":"How it Works"},{"location":"Using%20GitHub/","text":"Using Github Summary Github becomes a very useful file sharing and storage system. In Github one can upload local files to the internet for other people to access. Github will override old files with new files such that one can always use the latest versioin. Github uses a push and pull system to upload and download the file. To upload the files on Github, one can use the terminal on their computer. The first thing to do is to change the directory to the directory they want to upload on Github. Typically, one will have a folder on Github with the same name. Furthermore, one needs to clone that repository to Github. This can easily be done by copying the file path into Github where it says clone repository. Thereafter, they will type in their terminal (git add .) , then type (git commit) . After that, a screen will come up asking the user to name the upload. Type (i) to insert and then the user can type the name of the upload. To save and quit press (esc) then (:w) to save and (:q) to quit. Then type (git push) . this will update the git reposirtory and make all the updated work in that folder available to other people in the project. To download the files from Github, the user needs again to have a destination folder cloned as a Git repository on their computer. Then change the directory to that directory and type (git pull) and all the files from that repository will be on copied to their computer. It is important to note that a specific github location/user must be cloned to a specific directory on the computer. It is not like once a directory is a github repository for one user, that it is a github repository for all. Github is designed to work directly with members in group with several directories cloned for regular updating. R projects can also be cloned and directly linked to Github. On R Studio one can directly push project files to github without using the terminal. Intro The files necessary for using EnviroTyping are located in a repository; therefore, it will be increasingly helpful over time if you create your own account on GitHub . This allows you to easily access not only the EnviroTyping datasets available at the time of this writing, but also any changes made over time. If you have used GitHub previously, you can skim (or simply skip) this tutorial and jump to the information you need. However, if you are new to the whole experience following the below outline will provide you with a strong foundation to understand the tools GitHub provides. Creating the Account and Forking This first step is straight-forward. Once you arrive on GitHub , simply create an account by inputting a few identifying details, such as your email address. After your account is created you will be taken to your personal page. Here you can see your repositories, other projects you follow, and personal details. Feel free to customize it or add a picture because this account is yours, and becoming active on GitHub is a great networking opportunity as it connects you to the global research community. Now, direct your attention to the search bar on the top left-hand side of the portal. In order to find the EnviroTyping repository, search for \"TACC/EnviroTyping\". There should be only one result, so click on its name. A quick glance at the directory may be overwhelming at first, but do not worry about navigating the GitHub just yet. Instead, look near the top of the portal for an icon that says \"Fork\". Click on it to have control over your own copy of the EnviroTyping repository. Verify the repository was forked by clicking on your profile icon on the top right-hand side of the portal, going to \"Your Repositories\", and checking that EnviroTyping is an option in the list. This step is important because \"forking\" allows you to read and write the files in the master repository without fear of losing the original data. It also grants you the opportunity to contribute to the project by submitting \"Pull\" requests when you create (or edit) a file that helps further the project's goals. Navigating Directories As previously alluded to, there are a plethora of directories within the EnviroTyping GitHub. Needless to say, it can be confusing when trying to navigate the massive number of files. In order to alleviate some of the initial confusion when navigating the files, below are a couple of tables which briefly describe some important folders in the directory and imply whether or not they are commonly used. However, do keep in mind these directories may change and these tables are meant solely as a guide. The first table describes overall parent folders. Directory Description Commonly Used? .ipynb_checkpoints Holds one file No Premium @ 6750f8a Collection of files for PReMiuM package No Simulated-data-for-performance-testing Collection of files for simulated datasets No data Central hub for data pertaining to each year's hybrids Yes doc_files Collection of Markdown files for ReadtheDocs Yes Notes Various notes No References Collection of various articles for research Yes sandbox Collection of data and files in current use Yes tools Some benchmark functons No You will find yearly GxE hybrid data in the folder entitled \"data\". Immediately within this folder are two more: \"external\" and \"interim\". The former consists of files and raw data used to make the cleaned data in the \"interim\" folder. Often you will utilize the data found in the \"interim\" folder. Within it, you will see more folders subset by years. Within each year folder, you may see a small selection of datasets with different suffixes in their names. Each dataset is slightly different. For example, the files with the suffix \"wth_nas\" have missing observations; and those with \"shifted\" move observations from later months to earlier months to remove any such missing observations. Having so many datasets with such minute differences is necessary to our research as it helps us establish any inconsistencies across different statistical tests. It should be noted, however, the most commonly used files (and, consequently, most work) are found within \"sandbox\". Within \"sandbox\" are several folders, but below are the most frequently used. Directory Description posthoc_group_analysis Various years' files used to create the output in the \"Post hoc Anaylsis\" section of the ReadtheDocs shifted_data_analysis Various years' workflows with different workflows for minimum, mean/median, or maximum variables working_with_plots Collection of files utilized to make different figures to visualize EnviroTyping data Your specific tasks will determine where you find (and place) your work. Nonetheless, feel free to navigate the GitHub and utilize any file as needed. This part of the tutorial is meant solely to help you understand the basic structure of the EnviroTyping GitHub. Understanding the nature of the directories and where to find the most important files will help tremendously when you need to reference multiple datasets or simply find a figure. Setting EnviroTyping Project in RStudio Most of the code for EnviroTyping is written in R therefore, you should consider creating a \"Project\" in RStudio that you utilize when working with EnviroTyping files. Projects allow you to consistently work in a specific working directory on your local machine, easily connect to GitHub repositories, and streamline your work. If you do not already have RStudio downloaded on your computer, go to RStudio's website and select the installer for your OS. To first create a project you will need the URL of your forked EnviroTyping repository. In GitHub go to your EnviroTyping repository. Click on the green button that reads \"Clone or Download\" on the right-hand side of the page, then press the \"Copy\" button beside the URL that appears. Next, in RStudio go to \"Projects\" on the top right-hand side of the screen. Select \"New Project\", then \"Version Control\", then \"Git\". From here, paste the URL of your repository into the \"Repository URL\" field and type \"EnviroTyping\" as the project directory name. You may choose where you place the subdirector; this process will download the GitHub repository to your local machine, you are encouraged to choose a parent directory where you may easily find the EnviroTyping files. Finally, you may also select whether or not to have RStudio open a new session when you work in the Project (most opt for the new session). Now, the Project setup in RStudio is complete! When you need to use R for EnviroTyping tasks, simply open RStudio, go to \"Projects\", and select \"EnviroTyping\". From there, you can choose to navigate the EnviroTyping files in the directory on the bottom left-hand side of the screen (depending on your selected layout of RStudio), create new files, or do whatever else you need. Cloning Repository into Stampede You are almost finished! Now you need to ensure your portal on the Stampede servers has cloned the data from your personal GitHub fork of the EnviroTyping repository. Assuming you have already created an account with TACC, cloning your repository is straightforward. First, you must be connected to a secure Wi-Fi or ethernet network. If you are not, TACC will not allow you to connect to the Stampede servers. Next, open up Terminal (Mac OS/Linux) or Command Line (Windows), and log onto the servers by SSH. Type into your command line prompt ssh <TACC username>@stampede2.tacc.utexas.edu . After inputting your password the system will ask you to verify your login information by sending you a text or email, depending upon your settings, with a unique code. Inputting the code correctly will finalize the log-in process. Make sure you copy the URL from your forked repository, then type: git clone <repository URL> . The URL will be the same as the one used for the R Project previously. You will know your command was accepted if Terminal or Command Line tells you that files are being downloaded. Once your repository is cloned, you may now submit jobs to be processed on the HPC. If you are unfamiliar with the SLURM commands used to communicate with Stampede2, feel free to reference the \"Reminders and Useful Commands\" section of the ReadtheDocs.","title":"Introduction to GitHub and Project Set-up"},{"location":"Using%20GitHub/#using-github-summary","text":"Github becomes a very useful file sharing and storage system. In Github one can upload local files to the internet for other people to access. Github will override old files with new files such that one can always use the latest versioin. Github uses a push and pull system to upload and download the file. To upload the files on Github, one can use the terminal on their computer. The first thing to do is to change the directory to the directory they want to upload on Github. Typically, one will have a folder on Github with the same name. Furthermore, one needs to clone that repository to Github. This can easily be done by copying the file path into Github where it says clone repository. Thereafter, they will type in their terminal (git add .) , then type (git commit) . After that, a screen will come up asking the user to name the upload. Type (i) to insert and then the user can type the name of the upload. To save and quit press (esc) then (:w) to save and (:q) to quit. Then type (git push) . this will update the git reposirtory and make all the updated work in that folder available to other people in the project. To download the files from Github, the user needs again to have a destination folder cloned as a Git repository on their computer. Then change the directory to that directory and type (git pull) and all the files from that repository will be on copied to their computer. It is important to note that a specific github location/user must be cloned to a specific directory on the computer. It is not like once a directory is a github repository for one user, that it is a github repository for all. Github is designed to work directly with members in group with several directories cloned for regular updating. R projects can also be cloned and directly linked to Github. On R Studio one can directly push project files to github without using the terminal.","title":"Using Github Summary"},{"location":"Using%20GitHub/#intro","text":"The files necessary for using EnviroTyping are located in a repository; therefore, it will be increasingly helpful over time if you create your own account on GitHub . This allows you to easily access not only the EnviroTyping datasets available at the time of this writing, but also any changes made over time. If you have used GitHub previously, you can skim (or simply skip) this tutorial and jump to the information you need. However, if you are new to the whole experience following the below outline will provide you with a strong foundation to understand the tools GitHub provides.","title":"Intro"},{"location":"Using%20GitHub/#creating-the-account-and-forking","text":"This first step is straight-forward. Once you arrive on GitHub , simply create an account by inputting a few identifying details, such as your email address. After your account is created you will be taken to your personal page. Here you can see your repositories, other projects you follow, and personal details. Feel free to customize it or add a picture because this account is yours, and becoming active on GitHub is a great networking opportunity as it connects you to the global research community. Now, direct your attention to the search bar on the top left-hand side of the portal. In order to find the EnviroTyping repository, search for \"TACC/EnviroTyping\". There should be only one result, so click on its name. A quick glance at the directory may be overwhelming at first, but do not worry about navigating the GitHub just yet. Instead, look near the top of the portal for an icon that says \"Fork\". Click on it to have control over your own copy of the EnviroTyping repository. Verify the repository was forked by clicking on your profile icon on the top right-hand side of the portal, going to \"Your Repositories\", and checking that EnviroTyping is an option in the list. This step is important because \"forking\" allows you to read and write the files in the master repository without fear of losing the original data. It also grants you the opportunity to contribute to the project by submitting \"Pull\" requests when you create (or edit) a file that helps further the project's goals.","title":"Creating the Account and Forking"},{"location":"Using%20GitHub/#navigating-directories","text":"As previously alluded to, there are a plethora of directories within the EnviroTyping GitHub. Needless to say, it can be confusing when trying to navigate the massive number of files. In order to alleviate some of the initial confusion when navigating the files, below are a couple of tables which briefly describe some important folders in the directory and imply whether or not they are commonly used. However, do keep in mind these directories may change and these tables are meant solely as a guide. The first table describes overall parent folders. Directory Description Commonly Used? .ipynb_checkpoints Holds one file No Premium @ 6750f8a Collection of files for PReMiuM package No Simulated-data-for-performance-testing Collection of files for simulated datasets No data Central hub for data pertaining to each year's hybrids Yes doc_files Collection of Markdown files for ReadtheDocs Yes Notes Various notes No References Collection of various articles for research Yes sandbox Collection of data and files in current use Yes tools Some benchmark functons No You will find yearly GxE hybrid data in the folder entitled \"data\". Immediately within this folder are two more: \"external\" and \"interim\". The former consists of files and raw data used to make the cleaned data in the \"interim\" folder. Often you will utilize the data found in the \"interim\" folder. Within it, you will see more folders subset by years. Within each year folder, you may see a small selection of datasets with different suffixes in their names. Each dataset is slightly different. For example, the files with the suffix \"wth_nas\" have missing observations; and those with \"shifted\" move observations from later months to earlier months to remove any such missing observations. Having so many datasets with such minute differences is necessary to our research as it helps us establish any inconsistencies across different statistical tests. It should be noted, however, the most commonly used files (and, consequently, most work) are found within \"sandbox\". Within \"sandbox\" are several folders, but below are the most frequently used. Directory Description posthoc_group_analysis Various years' files used to create the output in the \"Post hoc Anaylsis\" section of the ReadtheDocs shifted_data_analysis Various years' workflows with different workflows for minimum, mean/median, or maximum variables working_with_plots Collection of files utilized to make different figures to visualize EnviroTyping data Your specific tasks will determine where you find (and place) your work. Nonetheless, feel free to navigate the GitHub and utilize any file as needed. This part of the tutorial is meant solely to help you understand the basic structure of the EnviroTyping GitHub. Understanding the nature of the directories and where to find the most important files will help tremendously when you need to reference multiple datasets or simply find a figure.","title":"Navigating Directories"},{"location":"Using%20GitHub/#setting-envirotyping-project-in-rstudio","text":"Most of the code for EnviroTyping is written in R therefore, you should consider creating a \"Project\" in RStudio that you utilize when working with EnviroTyping files. Projects allow you to consistently work in a specific working directory on your local machine, easily connect to GitHub repositories, and streamline your work. If you do not already have RStudio downloaded on your computer, go to RStudio's website and select the installer for your OS. To first create a project you will need the URL of your forked EnviroTyping repository. In GitHub go to your EnviroTyping repository. Click on the green button that reads \"Clone or Download\" on the right-hand side of the page, then press the \"Copy\" button beside the URL that appears. Next, in RStudio go to \"Projects\" on the top right-hand side of the screen. Select \"New Project\", then \"Version Control\", then \"Git\". From here, paste the URL of your repository into the \"Repository URL\" field and type \"EnviroTyping\" as the project directory name. You may choose where you place the subdirector; this process will download the GitHub repository to your local machine, you are encouraged to choose a parent directory where you may easily find the EnviroTyping files. Finally, you may also select whether or not to have RStudio open a new session when you work in the Project (most opt for the new session). Now, the Project setup in RStudio is complete! When you need to use R for EnviroTyping tasks, simply open RStudio, go to \"Projects\", and select \"EnviroTyping\". From there, you can choose to navigate the EnviroTyping files in the directory on the bottom left-hand side of the screen (depending on your selected layout of RStudio), create new files, or do whatever else you need.","title":"Setting EnviroTyping Project in RStudio"},{"location":"Using%20GitHub/#cloning-repository-into-stampede","text":"You are almost finished! Now you need to ensure your portal on the Stampede servers has cloned the data from your personal GitHub fork of the EnviroTyping repository. Assuming you have already created an account with TACC, cloning your repository is straightforward. First, you must be connected to a secure Wi-Fi or ethernet network. If you are not, TACC will not allow you to connect to the Stampede servers. Next, open up Terminal (Mac OS/Linux) or Command Line (Windows), and log onto the servers by SSH. Type into your command line prompt ssh <TACC username>@stampede2.tacc.utexas.edu . After inputting your password the system will ask you to verify your login information by sending you a text or email, depending upon your settings, with a unique code. Inputting the code correctly will finalize the log-in process. Make sure you copy the URL from your forked repository, then type: git clone <repository URL> . The URL will be the same as the one used for the R Project previously. You will know your command was accepted if Terminal or Command Line tells you that files are being downloaded. Once your repository is cloned, you may now submit jobs to be processed on the HPC. If you are unfamiliar with the SLURM commands used to communicate with Stampede2, feel free to reference the \"Reminders and Useful Commands\" section of the ReadtheDocs.","title":"Cloning Repository into Stampede"},{"location":"Using%20HPC/","text":"HPC (High Performance Computers) Summary As discussed in the Scanonevar.perm section, some functions when running giant data sets cannot be run on a home computer. Therefore, they need to be sent to an HPC. We use the TACC (Texas Advanced Computing Center). We are able to login to the HPC remotely and send files via githup to the HPC. We setup a github in the HPC and then our own Github then pull and push Why HPC? Sometimes data can get so complex or large that a personal computer is no longer a viable option to run the analysis. Luckily there are high performance computing clusters available which cater to different requirements, from raw CPU or GPU power to large memory needs. These clusters can be utilized to handle analyses using R, Python, or other languages. The National Science Foundation has partnered with universities and organizations around the country to create HPC clusters for students at universities to freely use (in moderation) via XSEDE . Necessary Accounts There are a couple platforms essential to the mission of the EnviroTyping research, namely Cyverse and XSEDE/TACC. You will be asked to create an account on both platforms. Cyverse Cyverse is an initiative led by the University of Arizona to improve how data is stored and shared amongst researchers. You may read more about its mission here and create an account on this page . XSEDE/TACC XSEDE is a virtual organization that serves to connect researchers to resources necessary to navigate the shear amount of data in the modern age. More background information is found on their About page. Create an account here . An XSEDE account will allow you to access any resource in the XSEDE network. Likewise, one such resource is the Texas Advanced Computing Center (TACC). As with the previously mentioned programs, more information is found on their About page. TACC holds the tools used to complete jobs too large to run on personal or local machines (such as the PReMiuM profile regressions discussed in the \"Workflow\" section of this ReadtheDocs). Specifically, you will most likely utilize the new Stampede2 servers. However, you must first create an account . It is encouraged you carefully read each page in the creation process thoroughly because TACC tends to be strict in its allocation of compute resources, and the more you know in the beginning the more it will help in the long run. We will discuss how to properly use Stampede2 in more detail, but feel free to briefly peruse its User Guide once your account is created.","title":"Getting Started With High Performance Computing"},{"location":"Using%20HPC/#hpc-high-performance-computers-summary","text":"As discussed in the Scanonevar.perm section, some functions when running giant data sets cannot be run on a home computer. Therefore, they need to be sent to an HPC. We use the TACC (Texas Advanced Computing Center). We are able to login to the HPC remotely and send files via githup to the HPC. We setup a github in the HPC and then our own Github then pull and push","title":"HPC (High Performance Computers) Summary"},{"location":"Using%20HPC/#why-hpc","text":"Sometimes data can get so complex or large that a personal computer is no longer a viable option to run the analysis. Luckily there are high performance computing clusters available which cater to different requirements, from raw CPU or GPU power to large memory needs. These clusters can be utilized to handle analyses using R, Python, or other languages. The National Science Foundation has partnered with universities and organizations around the country to create HPC clusters for students at universities to freely use (in moderation) via XSEDE .","title":"Why HPC?"},{"location":"Using%20HPC/#necessary-accounts","text":"There are a couple platforms essential to the mission of the EnviroTyping research, namely Cyverse and XSEDE/TACC. You will be asked to create an account on both platforms.","title":"Necessary Accounts"},{"location":"Using%20HPC/#cyverse","text":"Cyverse is an initiative led by the University of Arizona to improve how data is stored and shared amongst researchers. You may read more about its mission here and create an account on this page .","title":"Cyverse"},{"location":"Using%20HPC/#xsedetacc","text":"XSEDE is a virtual organization that serves to connect researchers to resources necessary to navigate the shear amount of data in the modern age. More background information is found on their About page. Create an account here . An XSEDE account will allow you to access any resource in the XSEDE network. Likewise, one such resource is the Texas Advanced Computing Center (TACC). As with the previously mentioned programs, more information is found on their About page. TACC holds the tools used to complete jobs too large to run on personal or local machines (such as the PReMiuM profile regressions discussed in the \"Workflow\" section of this ReadtheDocs). Specifically, you will most likely utilize the new Stampede2 servers. However, you must first create an account . It is encouraged you carefully read each page in the creation process thoroughly because TACC tends to be strict in its allocation of compute resources, and the more you know in the beginning the more it will help in the long run. We will discuss how to properly use Stampede2 in more detail, but feel free to briefly peruse its User Guide once your account is created.","title":"XSEDE/TACC"},{"location":"Using%20R/","text":"Using R R is a very convenient program for running cross analysis for large data pools. It is even more effective at running regression models with large matrices and tables. R is also very effective at reading in csv files and csv cross files. It also has many specific and quick table functions like tapply and sapply which are much more effeicient than using for loops. Finally, R is also the language in which the vQTL package is created in. R Studio R Studio is the IDE we use to run R in. It is much more user friendly that R. One can have several tabs open to work between or hold different programs. One can create new projects and link the project to Github for easy updating. It also remembers functions and variable names to increase speed of typing. Its help window on the bottom right hand corner is also very useful and can link to other options that one may not know exist. It can create plots effectively which can easily be saved onto a computer. Tidyverse package Using the tidyverse is not a requirement, but for data manipulation they provide standards that can be used across any type of R project and Hadley Wickham (the tidyverse core contributor) diligently researches best software development practices. Install via install.packages(\"tidyverse\") and check out the following free ebook by Hadley Wickham here R for Data Science . Running the library(tidyverse) command will load several packages used commonly like tidyr, dplyr, readr, and purrr.","title":"Getting Started"},{"location":"Using%20R/#using-r","text":"R is a very convenient program for running cross analysis for large data pools. It is even more effective at running regression models with large matrices and tables. R is also very effective at reading in csv files and csv cross files. It also has many specific and quick table functions like tapply and sapply which are much more effeicient than using for loops. Finally, R is also the language in which the vQTL package is created in.","title":"Using R"},{"location":"Using%20R/#r-studio","text":"R Studio is the IDE we use to run R in. It is much more user friendly that R. One can have several tabs open to work between or hold different programs. One can create new projects and link the project to Github for easy updating. It also remembers functions and variable names to increase speed of typing. Its help window on the bottom right hand corner is also very useful and can link to other options that one may not know exist. It can create plots effectively which can easily be saved onto a computer.","title":"R Studio"},{"location":"Using%20R/#tidyverse-package","text":"Using the tidyverse is not a requirement, but for data manipulation they provide standards that can be used across any type of R project and Hadley Wickham (the tidyverse core contributor) diligently researches best software development practices. Install via install.packages(\"tidyverse\") and check out the following free ebook by Hadley Wickham here R for Data Science . Running the library(tidyverse) command will load several packages used commonly like tidyr, dplyr, readr, and purrr.","title":"Tidyverse package"},{"location":"Workflow/","text":"Workflow Results Plots Conclusion","title":"Workflow"},{"location":"Workflow/#workflow","text":"","title":"Workflow"},{"location":"Workflow/#results","text":"","title":"Results"},{"location":"Workflow/#plots","text":"","title":"Plots"},{"location":"Workflow/#conclusion","text":"","title":"Conclusion"},{"location":"qPCR%20Analysis/","text":"qPCR Analysis Introduction Other headers Outputs Plots","title":"qPCR Analysis'"},{"location":"qPCR%20Analysis/#qpcr-analysis","text":"","title":"qPCR Analysis"},{"location":"qPCR%20Analysis/#introduction","text":"","title":"Introduction"},{"location":"qPCR%20Analysis/#other-headers","text":"","title":"Other headers"},{"location":"qPCR%20Analysis/#outputs","text":"","title":"Outputs"},{"location":"qPCR%20Analysis/#plots","text":"","title":"Plots"},{"location":"vQTL%20Analysis/","text":"vQTL Analysis and Scanonevar function Introduction The goal of the vQTL analysis is to run vQTL using the scanonevar functionon on the set of data of corn crops. This process will give the LODs and p-values of each respective gene for the mean, variance and mean-variance. The high LODs or low p-values are the genes that are most significant and most useful to the project. Scanonevar Function The Scanonevar function was developed by Robert Corty and is am extension of the Scanone function. It has the main goal of cross examining large data bases to to determine LODs and P-Values. The scanone function can easily be run on a local computer while the scanonevar function can take hours to run. Interactive model For our project we need the interactive model. The additive model, y = b0+b1x1+b2x2...bnxn is not accuarate enough for our model as we need the combinations on how each environmental factor interacts with each gene. So instead we are using the interactive model which is y = b0+b1x1+b2x2+b3x1x2. Scanonevar.perm Function This function runs numerous permutations of the scanonevar fucntion to identify the accuracy of the scanonevar function. This function, however, takes a lot longer to run and is not doable on a home computer and requires a super computer. We send these to TACC which is the HPC in Texas. Effectsizes function Outputs Plots","title":"vQTL Analysis'"},{"location":"vQTL%20Analysis/#vqtl-analysis-and-scanonevar-function","text":"","title":"vQTL Analysis and Scanonevar function"},{"location":"vQTL%20Analysis/#introduction","text":"The goal of the vQTL analysis is to run vQTL using the scanonevar functionon on the set of data of corn crops. This process will give the LODs and p-values of each respective gene for the mean, variance and mean-variance. The high LODs or low p-values are the genes that are most significant and most useful to the project.","title":"Introduction"},{"location":"vQTL%20Analysis/#scanonevar-function","text":"The Scanonevar function was developed by Robert Corty and is am extension of the Scanone function. It has the main goal of cross examining large data bases to to determine LODs and P-Values. The scanone function can easily be run on a local computer while the scanonevar function can take hours to run.","title":"Scanonevar Function"},{"location":"vQTL%20Analysis/#interactive-model","text":"For our project we need the interactive model. The additive model, y = b0+b1x1+b2x2...bnxn is not accuarate enough for our model as we need the combinations on how each environmental factor interacts with each gene. So instead we are using the interactive model which is y = b0+b1x1+b2x2+b3x1x2.","title":"Interactive model"},{"location":"vQTL%20Analysis/#scanonevarperm-function","text":"This function runs numerous permutations of the scanonevar fucntion to identify the accuracy of the scanonevar function. This function, however, takes a lot longer to run and is not doable on a home computer and requires a super computer. We send these to TACC which is the HPC in Texas.","title":"Scanonevar.perm Function"},{"location":"vQTL%20Analysis/#effectsizes-function","text":"","title":"Effectsizes function"},{"location":"vQTL%20Analysis/#outputs","text":"","title":"Outputs"},{"location":"vQTL%20Analysis/#plots","text":"","title":"Plots"}]}